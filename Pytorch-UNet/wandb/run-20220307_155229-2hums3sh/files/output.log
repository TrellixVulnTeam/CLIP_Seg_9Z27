INFO: Starting training:
        Epochs:          50
        Batch size:      256
        Learning rate:   1e-05
        Training size:   1464
        Validation size: 1449
        Checkpoints:     True
        Device:          cuda
        Images scaling:  0.5
        Mixed Precision: True
Epoch 1/50:   0%|                                                          | 0/1464 [00:00<?, ?img/s]
Epoch 1/50:   0%|                                                          | 0/1464 [00:03<?, ?img/s]
Traceback (most recent call last):
  File "train.py", line 355, in <module>
    train_net(net=torch.nn.DataParallel(net, device_ids=device_ids),
  File "train.py", line 204, in train_net
    R_image_temp = texts2r_image(class_name, voc12_template, device, clip_model, clip_img)
  File "/home/dginzburg/CLIP_Seg/Pytorch-UNet/utils/utils.py", line 86, in texts2r_image
    R_image_temp = interpret(model=clip_model, image=clip_img, texts=text, device=device)
  File "/home/dginzburg/CLIP_Seg/Pytorch-UNet/utils/utils.py", line 47, in interpret
    logits_per_image, logits_per_text = model(images, texts)
  File "/home/dginzburg/anaconda3/envs/CLIP_Seg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/dginzburg/CLIP_Seg/Pytorch-UNet/Transformer_MM_Explainability/CLIP/clip/model.py", line 365, in forward
    image_features = self.encode_image(image)
  File "/home/dginzburg/CLIP_Seg/Pytorch-UNet/Transformer_MM_Explainability/CLIP/clip/model.py", line 347, in encode_image
    return self.visual(image.type(self.dtype))
  File "/home/dginzburg/anaconda3/envs/CLIP_Seg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/dginzburg/CLIP_Seg/Pytorch-UNet/Transformer_MM_Explainability/CLIP/clip/model.py", line 230, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
  File "/home/dginzburg/anaconda3/envs/CLIP_Seg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/dginzburg/anaconda3/envs/CLIP_Seg/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 443, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/dginzburg/anaconda3/envs/CLIP_Seg/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 439, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [768, 3, 32, 32], expected input[1, 224, 224, 3] to have 3 channels, but got 224 channels instead