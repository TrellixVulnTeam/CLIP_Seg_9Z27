INFO: Starting training:
        Epochs:          50
        Batch size:      16
        Learning rate:   5e-07
        Training size:   1464
        Validation size: 1449
        Checkpoints:     True
        Device:          cuda
        Images scaling:  0.5
        Mixed Precision: True
Epoch 1/50:   0%|                                                          | 0/1464 [00:00<?, ?img/s]/home/dvir/anaconda3/envs/CLIP_Seg/lib/python3.8/site-packages/torch/nn/functional.py:3609: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/dvir/anaconda3/envs/CLIP_Seg/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /tmp/pip-req-build-pyrtjkcs/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)











Epoch 1/50:  31%|████████▊                    | 448/1464 [00:27<00:51, 19.62img/s, loss (batch)=1.16]/home/dvir/CLIP_Seg/Pytorch-UNet/utils/utils.py:77: RuntimeWarning: invalid value encountered in true_divide
  image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())























Epoch 1/50:  99%|███████████████████████████▊| 1456/1464 [01:17<00:00, 18.87img/s, loss (batch)=1.07]
Traceback (most recent call last):
  File "train.py", line 327, in <module>
    train_net(net=torch.nn.DataParallel(net, device_ids=list(map(int, args.device_ids.split(",")))),
  File "train.py", line 260, in train_net
    val_score = evaluate(net, val_loader, device, labels_dict, voc12_template, distractors_template, clip_model, clip_preprocess)
  File "/home/dvir/CLIP_Seg/Pytorch-UNet/evaluate.py", line 64, in evaluate
    R_image[i] = R_img_resize(R_image_temp)
  File "/home/dvir/CLIP_Seg/Pytorch-UNet/utils/utils.py", line 74, in R_img_resize
    image_relevance = image_relevance.reshape(1, 1, 7, 7)
RuntimeError: shape '[1, 1, 7, 7]' is invalid for input of size 98